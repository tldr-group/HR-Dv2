{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da280475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ronan/miniconda3/envs/dv2/lib/python3.12/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/home/ronan/miniconda3/envs/dv2/lib/python3.12/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "torch.cuda.empty_cache()\n",
    "from torch.nn.functional import interpolate\n",
    "\n",
    "from hr_dv2 import HighResDV2\n",
    "import hr_dv2.transform as tr\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time_ns\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "use_norm = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feb17993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(path: str, l: int) -> tuple[torch.Tensor, np.ndarray]:\n",
    "    temp_img = Image.open(path)\n",
    "    h, w = temp_img.height, temp_img.width\n",
    "    #transform = tr.closest_crop(h, w) #tr.get_input_transform(L, L)\n",
    "    transform = tr.get_input_transform(l, l)\n",
    "    tensor, img = tr.load_image(path, transform)\n",
    "    H, W = img.height, img.width\n",
    "    return tensor, np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "656e7a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_mem_time(inp: torch.Tensor, model: nn.Module, seq: bool = False) -> tuple[float, float]:\n",
    "    if type(model) == HighResDV2:\n",
    "        inp = inp.squeeze(0)\n",
    "\n",
    "    torch.cuda.reset_peak_memory_stats() # s.t memory is accurate\n",
    "    torch.cuda.synchronize() # s.t time is accurate\n",
    "    def _to_MB(x: int) -> float:\n",
    "        return x / (1024**2)\n",
    "\n",
    "    def _to_s(t: int) -> float:\n",
    "        return t / 1e9\n",
    "\n",
    "    start_m = torch.cuda.max_memory_allocated()\n",
    "    start_t = time_ns()\n",
    "    \n",
    "    if seq:\n",
    "        model.forward_sequential(inp)\n",
    "    else:\n",
    "        model.forward(inp)\n",
    "\n",
    "    end_m = torch.cuda.max_memory_allocated()\n",
    "    torch.cuda.synchronize()\n",
    "    end_t = time_ns()\n",
    "\n",
    "    return _to_MB(end_m - start_m), _to_s(end_t - start_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa8a7dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ronan/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/home/ronan/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/home/ronan/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/home/ronan/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "net = HighResDV2(\"dinov2_vits14_reg\", 4, dtype=torch.float16) #dino_vits8 #dinov2_vits14_reg\n",
    "net.interpolation_mode = 'nearest-exact'\n",
    "net.eval()\n",
    "net.cuda()\n",
    "net.half()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c46950e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor, img_arr = load_img('../fig_data/1.jpg', (350, 350))\n",
    "img_tensor = img_tensor.cuda().unsqueeze(0)\n",
    "img_tensor = img_tensor.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4669d920",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    net.forward(img_tensor.squeeze(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1588e7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH\n",
      "\n",
      "\tno_trs, N_t=0: 281.82 MB, 0.02 s\n",
      "\tmoore_1, N_t=9: 551.13 MB, 0.15 s\n",
      "\tneumann_1, N_t=5: 505.99 MB, 0.09 s\n",
      "\tmoore_2, N_t=17: 1002.16 MB, 0.28 s\n",
      "\tmoore_2_flip, N_t=68: 4008.97 MB, 1.12 s\n",
      "\tmoore_4, N_t=33: 1945.37 MB, 0.54 s\n",
      "\tmoore_4_flip, N_t=132: 7781.51 MB, 2.18 s\n",
      "SEQUENTIAL\n",
      "\n",
      "\tno_trs, N_t=1: 281.82 MB, 0.02 s\n",
      "\tmoore_1, N_t=9: 466.34 MB, 0.19 s\n",
      "\tneumann_1, N_t=5: 463.91 MB, 0.11 s\n",
      "\tmoore_2, N_t=17: 472.48 MB, 0.36 s\n",
      "\tmoore_2_flip, N_t=68: 508.56 MB, 1.46 s\n",
      "\tmoore_4, N_t=33: 483.70 MB, 0.70 s\n",
      "\tmoore_4_flip, N_t=132: 553.09 MB, 2.84 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "flip = tr.get_flip_transforms()\n",
    "no_trs = ([], [])\n",
    "moore_1 = tr.get_shift_transforms([1], 'Moore')\n",
    "neumann_1 = tr.get_shift_transforms([1], 'Neumann')\n",
    "moore_2 = tr.get_shift_transforms([1, 2], 'Moore')\n",
    "moore_4 = tr.get_shift_transforms([1, 2, 3, 4], 'Moore')\n",
    "\n",
    "moore_2_flip = tr.combine_transforms(moore_2[0], flip[0], moore_2[1], flip[1])\n",
    "moore_4_flip = tr.combine_transforms(moore_4[0], flip[0], moore_4[1], flip[1])\n",
    "\n",
    "\n",
    "names = ['no_trs', 'moore_1', 'neumann_1' , 'moore_2', 'moore_2_flip', 'moore_4', 'moore_4_flip']\n",
    "fwd_inv_transforms: list[tuple[tr.PartialTrs, tr.PartialTrs]] = [no_trs, moore_1, neumann_1, moore_2, moore_2_flip, moore_4, moore_4_flip]\n",
    "\n",
    "for is_seq in (False, True):\n",
    "    prefix = \"Sequential\" if is_seq else \"Batch\"\n",
    "    print(f\"{prefix.upper()}\\n\")\n",
    "    for i, (fwd, inv) in enumerate(fwd_inv_transforms):\n",
    "        n_t = len(fwd)\n",
    "        net.set_transforms(fwd, inv)\n",
    "        mem, time = measure_mem_time(img_tensor, net, is_seq)\n",
    "        print(f\"\\t{names[i]}, N_t={n_t}: {mem:.2f} MB, {time:.2f} s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d25023d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ronan/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7959.06201171875 2.192014303\n"
     ]
    }
   ],
   "source": [
    "dino_net = HighResDV2(\"dino_vits8\", 4, dtype=torch.float16)\n",
    "dino_net.interpolation_mode = 'nearest-exact'\n",
    "dino_net.eval()\n",
    "dino_net.cuda()\n",
    "dino_net.half()\n",
    "\n",
    "dino_net.set_transforms(fwd, inv)\n",
    "mem, time = measure_mem_time(img_tensor, dino_net, False)\n",
    "print(mem, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4eb39458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH\n",
      "\n",
      "\tno_trs, N_t=1: 281.54 MB, 0.03 s\n",
      "\tmoore_1, N_t=9: 552.91 MB, 0.15 s\n",
      "\tneumann_1, N_t=5: 506.85 MB, 0.08 s\n",
      "\tmoore_2, N_t=17: 1025.03 MB, 0.28 s\n",
      "\tmoore_2_flip, N_t=68: 4100.46 MB, 1.12 s\n",
      "\tmoore_4, N_t=33: 1989.77 MB, 0.54 s\n",
      "\tmoore_4_flip, N_t=132: 7959.06 MB, 2.19 s\n",
      "SEQUENTIAL\n",
      "\n",
      "\tno_trs, N_t=1: 281.54 MB, 0.02 s\n",
      "\tmoore_1, N_t=9: 467.11 MB, 0.19 s\n",
      "\tneumann_1, N_t=5: 464.11 MB, 0.10 s\n",
      "\tmoore_2, N_t=17: 472.52 MB, 0.36 s\n",
      "\tmoore_2_flip, N_t=68: 508.28 MB, 1.43 s\n",
      "\tmoore_4, N_t=33: 483.74 MB, 0.69 s\n",
      "\tmoore_4_flip, N_t=132: 552.81 MB, 2.80 s\n"
     ]
    }
   ],
   "source": [
    "for is_seq in (False, True):\n",
    "    prefix = \"Sequential\" if is_seq else \"Batch\"\n",
    "    print(f\"{prefix.upper()}\\n\")\n",
    "    for i, (fwd, inv) in enumerate(fwd_inv_transforms):\n",
    "        n_t = len(fwd)\n",
    "        dino_net.set_transforms(fwd, inv)\n",
    "        mem, time = measure_mem_time(img_tensor, dino_net, is_seq)\n",
    "        print(f\"\\t{names[i]}, N_t={n_t}: {mem:.2f} MB, {time:.2f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c34ed7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ronan/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    }
   ],
   "source": [
    "net = HighResDV2(\"dinov2_vitb14_reg\", 4, dtype=torch.float16) #dino_vits8 #dinov2_vits14_reg\n",
    "net.interpolation_mode = 'nearest-exact'\n",
    "net.eval()\n",
    "net.cuda()\n",
    "net.half()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e69c60a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH\n",
      "\n",
      "\tno_trs, N_t=0: 560.21 MB, 0.04 s\n",
      "\tmoore_1, N_t=9: 1094.16 MB, 0.34 s\n",
      "\tneumann_1, N_t=5: 1006.63 MB, 0.19 s\n",
      "\tmoore_2, N_t=17: 1992.40 MB, 0.65 s\n",
      "\tmoore_2_flip, N_t=68: 7969.60 MB, 2.63 s\n",
      "\tmoore_4, N_t=33: 3867.60 MB, 1.25 s\n",
      "\tmoore_4_flip, N_t=132: 15470.51 MB, 5.17 s\n",
      "SEQUENTIAL\n",
      "\n",
      "\tno_trs, N_t=1: 560.21 MB, 0.04 s\n",
      "\tmoore_1, N_t=9: 924.71 MB, 0.41 s\n",
      "\tneumann_1, N_t=5: 921.90 MB, 0.23 s\n",
      "\tmoore_2, N_t=17: 930.31 MB, 0.78 s\n",
      "\tmoore_2_flip, N_t=68: 966.06 MB, 3.15 s\n",
      "\tmoore_4, N_t=33: 941.53 MB, 1.52 s\n",
      "\tmoore_4_flip, N_t=132: 1010.92 MB, 6.15 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "flip = tr.get_flip_transforms()\n",
    "no_trs = ([], [])\n",
    "moore_1 = tr.get_shift_transforms([1], 'Moore')\n",
    "neumann_1 = tr.get_shift_transforms([1], 'Neumann')\n",
    "moore_2 = tr.get_shift_transforms([1, 2], 'Moore')\n",
    "moore_4 = tr.get_shift_transforms([1, 2, 3, 4], 'Moore')\n",
    "\n",
    "moore_2_flip = tr.combine_transforms(moore_2[0], flip[0], moore_2[1], flip[1])\n",
    "moore_4_flip = tr.combine_transforms(moore_4[0], flip[0], moore_4[1], flip[1])\n",
    "\n",
    "\n",
    "names = ['no_trs', 'moore_1', 'neumann_1' , 'moore_2', 'moore_2_flip', 'moore_4', 'moore_4_flip']\n",
    "fwd_inv_transforms: list[tuple[tr.PartialTrs, tr.PartialTrs]] = [no_trs, moore_1, neumann_1, moore_2, moore_2_flip, moore_4, moore_4_flip]\n",
    "\n",
    "for is_seq in (False, True):\n",
    "    prefix = \"Sequential\" if is_seq else \"Batch\"\n",
    "    print(f\"{prefix.upper()}\\n\")\n",
    "    for i, (fwd, inv) in enumerate(fwd_inv_transforms):\n",
    "        n_t = len(fwd)\n",
    "        net.set_transforms(fwd, inv)\n",
    "        mem, time = measure_mem_time(img_tensor, net, is_seq)\n",
    "        print(f\"\\t{names[i]}, N_t={n_t}: {mem:.2f} MB, {time:.2f} s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
